# Assignment 3
This repository contains the codes and reports for the assignment 3 of the course CS6910 - Fundamentals of Deep Learning, conducted by Prof Mitesh Khapra at IIT Madras

# Overall structure of Assignment 
Below is the overall structure of our assignment works. The image has been generated with the help of wandb </br>
![nazarandaaz](https://github.com/vishnukt2506/Assignment_3/blob/main/model.png)
# Use recurrent neural networks to build a transliteration system

Using Recurrent Neural Networks, we build a poject here, which implements a sequence to sequence learning, Wen have also done comparing different cells such as vanilla RNN, LSTM and GRU and understanding how attention networks overcome the limitations of vanilla seq2seq models and have visualised the interactions between different components in a RNN based model. 

# The use of wandb

We have used the wandb for hyperparameter configuration, It can also be used to generate automatic plots and the same can be cloned to our wandb reports. It was helpful in visualizing the training, organizing and comparing lots of training runs and can also be used to analyze our models.

# Wandb report
Following is the link to our Wandb reports.
https://wandb.ai/dl_assignments/assignment-4/reports/Assignment-3--VmlldzoxOTY4NTkx
It contains the insightful observations made during the training.

# Future scope
The same codes can be used for performing experiments on larger datasets.
It can also be used for different datasets that are similar.
## **Team Members :**
Vishnu K T - OE21S024
Chandru N - CS20M041

## Languages and packages
1. keras
2. Tensorflow
3. Matplotlib
4. Numpy
5. Pandas

## Code editor and Compiler
We used **Google Colab**, an online real-time collaborative code editor and compiler for the web by enabling GPUs, which is much easier than running it on a local machine.

## Acknowledgements
1. Youtube playlist of Associate Proffessor Mitesh Khapra, Indian Institute of Technology, Madras - https://www.youtube.com/watch?v=aPfkYu_qiF4&list=PLEAYkSg4uSQ1r2XrJ_GBzzS6I-f8yfRU
2. Blog in Analytics Vidhya for RNN - https://www.analyticsvidhya.com/blog/2017/12/introduction-to-recurrent-neuralnetworks/Blog on dropout for regularizing –
3. Blog for Dropout – https://machinelearningmastery.com/dropout-for-regularizingdeep-neural-networks/
4. Additional knowledge on basics of CNN - https://datascience.stackexchange.com/questions/64022/
5. Pre-training vs Fine tuning -- https://stackoverflow.com/questions/68461204/continual-pre-training-vs-finetuning-a-language-model-with-mlm
6. Training in Keras – https://stackoverflow.com/questions/52031587/how-can-i-make-a-trainableparameter-in-keras
7. Guide to RNN Blog – https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks79e5eb8049c9





