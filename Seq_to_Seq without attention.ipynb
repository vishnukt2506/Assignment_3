{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "arup.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c98b615c90164f12a27b6150c7ccbcd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cdf149a265d44480a28cac7573fbc0d6",
              "IPY_MODEL_ee3b9996386f4be4bc94130557d2786f"
            ],
            "layout": "IPY_MODEL_e5d91d14374e4fa1b95e81f847d9171b"
          }
        },
        "cdf149a265d44480a28cac7573fbc0d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86e6302e589d41f88510c88bfa378943",
            "placeholder": "​",
            "style": "IPY_MODEL_1077e278b80b4a14b6fc50fc9a13bbbd",
            "value": "0.011 MB of 0.011 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "ee3b9996386f4be4bc94130557d2786f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a68900be4ae4f4587129d4cd98eaf1f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_932526099f0e46c3adce6f30b343de7d",
            "value": 1
          }
        },
        "e5d91d14374e4fa1b95e81f847d9171b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86e6302e589d41f88510c88bfa378943": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1077e278b80b4a14b6fc50fc9a13bbbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a68900be4ae4f4587129d4cd98eaf1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "932526099f0e46c3adce6f30b343de7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3s8QXeLI1X8R"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar --output daksh.tar"
      ],
      "metadata": {
        "id": "0_kzMijr1lKI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13c74ab4-5251-4790-de18-1f5e729e13c8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1915M  100 1915M    0     0   154M      0  0:00:12  0:00:12 --:--:-- 97.3M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!tar -xvf  'daksh.tar'"
      ],
      "metadata": {
        "id": "j4IEIAmK1n1J"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data(path,input_tokenizer=None,target_tokenizer=None,input_length=None,target_length=None):\n",
        "  input_texts = []\n",
        "  target_texts = []\n",
        "  \n",
        "  df = pd.read_csv(path,sep=\"\\t\",names=[\"1\", \"2\",\"3\"]).astype(str)\n",
        "  if input_tokenizer is None:\n",
        "      df=df.sample(frac=1)\n",
        "  # Add all the  input and target texts with start sequence and end sequence added to target \n",
        "  for index, row in df.iterrows():\n",
        "      input_text=row['2']\n",
        "      target_text= row['1']\n",
        "      if target_text =='</s>' or input_text=='</s>':\n",
        "        continue\n",
        "      target_text = \"\\t\" + target_text + \"\\n\"\n",
        "      input_texts.append(input_text)\n",
        "      target_texts.append(target_text)\n",
        "  \n",
        "  #only train set will have input_tokenizer as none. Validation and test will will use the same.\n",
        "  if input_tokenizer is None:\n",
        "    input_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', char_level=True)\n",
        "    input_tokenizer.fit_on_texts(input_texts)\n",
        "  input_tensor = input_tokenizer.texts_to_sequences(input_texts)\n",
        "  input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor,padding='post')\n",
        "  if target_tokenizer is None:\n",
        "    target_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', char_level=True)\n",
        "    target_tokenizer.fit_on_texts(target_texts)\n",
        "  #tokenize the text\n",
        "  target_tensor = target_tokenizer.texts_to_sequences(target_texts)\n",
        "  #pad the text\n",
        "  target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor,padding='post')\n",
        "  #for dataset which is not training we pad to make maximum length same as train set.\n",
        "  if input_length is not None and target_length is not None:\n",
        "      input_tensor=tf.concat([input_tensor,tf.zeros((input_tensor.shape[0],input_length-input_tensor.shape[1]))],axis=1)\n",
        "      target_tensor=tf.concat([target_tensor,tf.zeros((target_tensor.shape[0],target_length-target_tensor.shape[1]))],axis=1)\n",
        "  return input_texts,input_tensor,input_tokenizer,target_texts,target_tensor,target_tokenizer"
      ],
      "metadata": {
        "id": "dYeRwAbZ1qDL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "input_texts,input_tensor,input_tokenizer,target_texts,target_tensor,target_tokenizer=data(\"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")"
      ],
      "metadata": {
        "id": "rUkRZoWZ12oI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "val_input_texts,val_input_tensor,val_input_tokenizer,val_target_texts,val_target_tensor,val_target_tokenizer=data(\"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\",input_tokenizer,target_tokenizer,input_tensor.shape[1],target_tensor.shape[1])"
      ],
      "metadata": {
        "id": "mMZkUqVX19uB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "test_input_texts,test_input_tensor,test_input_tokenizer,test_target_texts,test_target_tensor,test_target_tokenizer=data(\"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\",input_tokenizer,target_tokenizer,input_tensor.shape[1],target_tensor.shape[1])"
      ],
      "metadata": {
        "id": "4PXPk1pb2B-Y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_encoder_tokens = len(input_tokenizer.word_index)+1\n",
        "num_decoder_tokens = len(target_tokenizer.word_index)+1\n",
        "max_encoder_seq_length =  input_tensor.shape[1]\n",
        "max_decoder_seq_length = target_tensor.shape[1]"
      ],
      "metadata": {
        "id": "zBqd2zmt2KcR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert index to character\n",
        "index_to_char_input = dict((input_tokenizer.word_index[key], key) for key in input_tokenizer.word_index.keys())\n",
        "index_to_char_target = dict((target_tokenizer.word_index[key], key) for key in target_tokenizer.word_index.keys())"
      ],
      "metadata": {
        "id": "IGTSDdW32NQQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the model\n",
        "def build_model(rnn_type,embedding_dim,encoder_layers,decoder_layers,dropout):\n",
        "  #input layer ; takes in tokenize input\n",
        "  encoder_inputs = keras.Input(shape=( max_encoder_seq_length))\n",
        "  #embedding layer\n",
        "  embed = keras.layers.Embedding(num_encoder_tokens, embedding_dim)(encoder_inputs)\n",
        "  #will store output of last added layer so that we can add multiple layers\n",
        "  last_encoder=None\n",
        "  if rnn_type=='LSTM':\n",
        "    #adding everything except the last LSTM layer, because in last layer return state=True\n",
        "    for i in range(encoder_layers-1):\n",
        "      encoder = keras.layers.LSTM(latent_dim, return_sequences=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        encoder_out = encoder(embed)\n",
        "      else:\n",
        "        encoder_out = encoder(last_encoder)\n",
        "      last_encoder=encoder_out\n",
        "    #last LSTM Layer\n",
        "    encoder = keras.layers.LSTM(latent_dim, return_state=True,dropout=dropout)\n",
        "    #handling the corner case, when there is only one LSTM layer.The above loop won't run.\n",
        "    if encoder_layers == 1:\n",
        "      encoder_outputs, state_h, state_c = encoder(embed)\n",
        "    else:\n",
        "      encoder_outputs, state_h, state_c = encoder(last_encoder)\n",
        "    #storing the hidden states only\n",
        "    encoder_states = [state_h, state_c]\n",
        "  elif rnn_type=='GRU':\n",
        "    #adding everything except the last GRU layer, because in last layer return state=True    \n",
        "    for i in range(encoder_layers-1):\n",
        "      encoder = keras.layers.GRU(latent_dim, return_sequences=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        encoder_out = encoder(embed)\n",
        "      else:\n",
        "        encoder_out = encoder(last_encoder)\n",
        "      last_encoder=encoder_out\n",
        "    #last GRU Layer\n",
        "    encoder = keras.layers.GRU(latent_dim, return_state=True,dropout=dropout)\n",
        "    #handling the corner case, when there is only one GRU layer.The above loop won't run\n",
        "    if encoder_layers == 1:\n",
        "      encoder_outputs, state = encoder(embed)\n",
        "    else:\n",
        "      encoder_outputs, state = encoder(last_encoder)\n",
        "    encoder_states = [state]\n",
        "  elif rnn_type=='RNN':\n",
        "    #adding everything except the last RNN layer, because in last layer return state=True\n",
        "    for i in range(encoder_layers-1):      \n",
        "      encoder = keras.layers.SimpleRNN(latent_dim, return_sequences=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        encoder_out = encoder(embed)\n",
        "      else:\n",
        "        encoder_out = encoder(last_encoder)\n",
        "      last_encoder=encoder_out\n",
        "    #last RNN Layer\n",
        "    encoder = keras.layers.SimpleRNN(latent_dim, return_state=True,dropout=dropout)\n",
        "    #handling the corner case, when there is only one RNN layer.The above loop won't run\n",
        "    if encoder_layers == 1:\n",
        "      encoder_outputs, state = encoder(embed)\n",
        "    else:\n",
        "      encoder_outputs, state = encoder(last_encoder)\n",
        "    encoder_states = [state]  \n",
        "\n",
        "\n",
        "  decoder_inputs = keras.Input(shape=( max_decoder_seq_length))\n",
        "  embed = keras.layers.Embedding(num_decoder_tokens, embedding_dim)(decoder_inputs)\n",
        "\n",
        "  if rnn_type==\"LSTM\":\n",
        "    #add all the LSTM layers\n",
        "    for i in range(decoder_layers):\n",
        "      decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        decoder_outputs, _, _ = decoder_lstm(embed, initial_state=encoder_states)\n",
        "      else:  \n",
        "        decoder_outputs, _, _ = decoder_lstm(last, initial_state=encoder_states)\n",
        "      last=decoder_outputs\n",
        "    #Adding dense layer at the end\n",
        "    decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\",name='final')\n",
        "    decoder_outputs = decoder_dense(last)\n",
        "  elif rnn_type==\"GRU\":\n",
        "    #add all the GRU layers\n",
        "    for i in range(decoder_layers):\n",
        "      decoder_lstm = keras.layers.GRU(latent_dim, return_sequences=True, return_state=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        decoder_outputs, _= decoder_lstm(embed, initial_state=encoder_states)\n",
        "      else:  \n",
        "        decoder_outputs, _ = decoder_lstm(last, initial_state=encoder_states)\n",
        "      last=decoder_outputs\n",
        "    #Adding dense layer at the end\n",
        "    decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\",name='final')\n",
        "    decoder_outputs = decoder_dense(last)\n",
        "  elif rnn_type==\"RNN\":\n",
        "    #add all the RNN layers\n",
        "    for i in range(decoder_layers):\n",
        "      decoder_lstm = keras.layers.SimpleRNN(latent_dim, return_sequences=True, return_state=True,dropout=dropout)\n",
        "      if i==0:\n",
        "        decoder_outputs, _= decoder_lstm(embed, initial_state=encoder_states)\n",
        "      else:  \n",
        "        decoder_outputs, _ = decoder_lstm(last, initial_state=encoder_states)\n",
        "      last=decoder_outputs\n",
        "    #Adding dense layer at the end\n",
        "    decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\",name='final')\n",
        "    decoder_outputs = decoder_dense(last)\n",
        "  #specifying model inputs and outputs.\n",
        "  # encoder_inputs -> Input to encoder\n",
        "  # decoder_inputs -> Input to decoder for teacher forcing\n",
        "  # decoder_outputs -> Output\n",
        "  model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "  return model"
      ],
      "metadata": {
        "id": "n67VhxAg2UbV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "def build_inference(model,encoder_layers,decoder_layers):\n",
        "    encoder_inputs = model.input[0]  \n",
        "    if isinstance(model.layers[encoder_layers+3], keras.layers.LSTM):\n",
        "      encoder_outputs, state_h_enc, state_c_enc = model.layers[encoder_layers+3].output  \n",
        "      encoder_states = [state_h_enc, state_c_enc]\n",
        "    elif isinstance(model.layers[encoder_layers+3], keras.layers.GRU):\n",
        "      encoder_outputs, state = model.layers[encoder_layers+3].output  \n",
        "      encoder_states = [state]\n",
        "    elif isinstance(model.layers[encoder_layers+3], keras.layers.RNN):\n",
        "      encoder_outputs, state = model.layers[encoder_layers+3].output  \n",
        "      encoder_states = [state]\n",
        "    encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "    decoder_inputs =  keras.Input(shape=( 1))  \n",
        "    if isinstance(model.layers[encoder_layers+3], keras.layers.LSTM):\n",
        "      decoder_states_inputs=[]\n",
        "      decoder_states=[]\n",
        "      last=None\n",
        "      for i in range(decoder_layers):\n",
        "        #every layer must have an input through which we can supply it's hidden state\n",
        "        decoder_state_input_h = keras.Input(shape=(latent_dim,),name='inp3_'+str(i))\n",
        "        decoder_state_input_c = keras.Input(shape=(latent_dim,),name='inp4_'+str(i))\n",
        "        x = [decoder_state_input_h, decoder_state_input_c]\n",
        "        decoder_lstm = model.layers[i+encoder_layers+4]\n",
        "        if i==0:\n",
        "          decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "              model.layers[i+encoder_layers+2](decoder_inputs), initial_state=x\n",
        "          )\n",
        "        else:\n",
        "          decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "              last, initial_state=x \n",
        "          )\n",
        "        last=decoder_outputs\n",
        "        decoder_states_inputs.append (decoder_state_input_h)\n",
        "        decoder_states_inputs.append (decoder_state_input_c)\n",
        "        decoder_states.append (state_h_dec)\n",
        "        decoder_states.append (state_c_dec)\n",
        "    elif isinstance(model.layers[encoder_layers+3], keras.layers.GRU):\n",
        "      decoder_states_inputs=[] #Contain all input layers for different GRU's hidden state\n",
        "      decoder_states=[] #Contains the hidden states\n",
        "      last=None\n",
        "      for i in range(decoder_layers):\n",
        "        decoder_state_input = keras.Input(shape=(latent_dim,),name='inp3_'+str(i))\n",
        "        x = [decoder_state_input]\n",
        "        decoder_lstm = model.layers[i+encoder_layers+4]\n",
        "        if i==0:\n",
        "          decoder_outputs, state = decoder_lstm(\n",
        "              model.layers[i+encoder_layers+2](decoder_inputs), initial_state=x\n",
        "          )\n",
        "        else:\n",
        "          decoder_outputs, state = decoder_lstm(\n",
        "              last, initial_state=x \n",
        "          )\n",
        "        last=decoder_outputs\n",
        "        decoder_states_inputs.append (decoder_state_input)\n",
        "        decoder_states.append (state)\n",
        "    elif isinstance(model.layers[encoder_layers+3], keras.layers.RNN):\n",
        "      decoder_states_inputs=[]\n",
        "      decoder_states=[]\n",
        "      last=None\n",
        "      for i in range(decoder_layers):\n",
        "        decoder_state_input = keras.Input(shape=(latent_dim,),name='inp3_'+str(i))\n",
        "        x = [decoder_state_input]\n",
        "        decoder_lstm = model.layers[i+encoder_layers+4]\n",
        "        if i==0:\n",
        "          decoder_outputs, state = decoder_lstm(\n",
        "              model.layers[i+encoder_layers+2](decoder_inputs), initial_state=x\n",
        "          )\n",
        "        else:\n",
        "          decoder_outputs, state = decoder_lstm(\n",
        "              last, initial_state=x \n",
        "          )\n",
        "        last=decoder_outputs\n",
        "        decoder_states_inputs.append (decoder_state_input)\n",
        "        decoder_states.append (state)      \n",
        "    decoder_dense = model.get_layer('final')\n",
        "    decoder_outputs = decoder_dense(last)\n",
        "    decoder_model = keras.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        "    )\n",
        "    return encoder_model,decoder_model"
      ],
      "metadata": {
        "id": "pGEVql012ZQ1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_batch(input_seq,encoder_model,decoder_model,batch_size,encoder_layers,decoder_layers):\n",
        "    # Get encoder output\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    if cell=='GRU' or 'RNN':\n",
        "      states_value=[states_value]\n",
        "    nl=states_value\n",
        "    for i in range(decoder_layers-1):\n",
        "      nl=nl+states_value\n",
        "    states_value=nl\n",
        "    \n",
        "    # This is contain previously predicted character's index for every words in batch.\n",
        "    prev_char_index = np.zeros((batch_size, 1))\n",
        "    # We start with \\t for every word in batch\n",
        "    prev_char_index[:, 0] = target_tokenizer.word_index['\\t']\n",
        "    \n",
        "    predicted_words = [ \"\" for i in range(batch_size)]\n",
        "    done=[False for i in range(batch_size)]\n",
        "    for i in range(max_decoder_seq_length):\n",
        "        out = decoder_model.predict(tuple([prev_char_index] + states_value))\n",
        "        output_probability=out[0]\n",
        "        states_value = out[1:]\n",
        "        for j in range(batch_size):\n",
        "          if done[j]:\n",
        "            continue          \n",
        "          sampled_token_index = np.argmax(output_probability[j, -1, :])\n",
        "          if sampled_token_index == 0:\n",
        "            sampled_char='\\n'\n",
        "          else:\n",
        "            sampled_char = index_to_char_target[sampled_token_index]\n",
        "          if sampled_char == '\\n':\n",
        "            done[j]=True\n",
        "            continue            \n",
        "          predicted_words[j] += sampled_char\n",
        "          #update the previously predicted characters        \n",
        "          prev_char_index[j,0]=target_tokenizer.word_index[sampled_char]\n",
        "    return predicted_words"
      ],
      "metadata": {
        "id": "CvmjYrd72cqQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_accuracy(encoder_model,decoder_model,encoder_layers,decoder_layers):\n",
        "  success=0\n",
        "  #Get all the predicted words\n",
        "  pred=decode_batch(test_input_tensor,encoder_model,decoder_model,test_input_tensor.shape[0],encoder_layers,decoder_layers)\n",
        "  for seq_index in range(test_input_tensor.shape[0]):\n",
        "      predicted_word = pred[seq_index]\n",
        "      target_word=test_target_texts[seq_index][1:-1]\n",
        "      #test the word one by one and write to files\n",
        "      if target_word == predicted_word:\n",
        "        success+=1\n",
        "        f = open(\"success.txt\", \"a\")\n",
        "        f.write(test_input_texts[seq_index]+' '+target_word+' '+predicted_word+'\\n')\n",
        "        f.close()\n",
        "      else:\n",
        "        f = open(\"failure.txt\", \"a\")\n",
        "        f.write(test_input_texts[seq_index]+' '+target_word+' '+predicted_word+'\\n')\n",
        "        f.close()\n",
        "  return float(success)/float(test_input_tensor.shape[0])"
      ],
      "metadata": {
        "id": "ErX4Lr4W2f49"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_validate(encoder_model,decoder_model,encoder_layers,decoder_layers):\n",
        "  success=0\n",
        "  #get all the predicted words\n",
        "  pred=decode_batch(val_input_tensor,encoder_model,decoder_model,val_input_tensor.shape[0],encoder_layers,decoder_layers)\n",
        "  for seq_index in range(val_input_tensor.shape[0]):\n",
        "      predicted_word = pred[seq_index]\n",
        "      target_word=val_target_texts[seq_index][1:-1]\n",
        "      #test the words one by one\n",
        "      if predicted_word == target_word:\n",
        "        success+=1\n",
        "  return float(success)/float(val_input_tensor.shape[0])"
      ],
      "metadata": {
        "id": "Ic2e9Etj2jIt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install wandb\n",
        "wb = True"
      ],
      "metadata": {
        "id": "G97t8zox2ymT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_type=None\n",
        "embedding_dim=None\n",
        "model= None\n",
        "latent_dim = None\n",
        "enc_layers=None\n",
        "dec_layers=None\n",
        "def train():\n",
        "  global rnn_type\n",
        "  global embedding_dim\n",
        "  global model\n",
        "  global latent_dim\n",
        "  global enc_layer\n",
        "  global dec_layer\n",
        "  wandb.init()\n",
        "  rnn_type=wandb.config.cell\n",
        "  embedding_dim=wandb.config.Embedding\n",
        "  latent_dim=wandb.config.Latent\n",
        "  enc_layer=wandb.config.Encoder_layer\n",
        "  dec_layer=wandb.config.Decoder_layer\n",
        "  dropout=wandb.config.dropout\n",
        "  epochs=wandb.config.epochs\n",
        "  bs=wandb.config.Batch_size\n",
        "  wandb.run.name = 'epochs_'+str(epochs)+'_bs_'+str(bs)+'_rnn_type_'+str(rnn_type)+'_em_'+str(embedding_dim)+'_latd_'+str(latent_dim)+'_encs_'+str(enc_layer)+'_decs_'+str(dec_layer)+'_dr_'+str(dropout)\n",
        "\n",
        "\n",
        "  model=build_model(rnn_type=rnn_type,embedding_dim=embedding_dim,encoder_layers=enc_layer,decoder_layers=dec_layer,dropout=.1)\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=\"adam\", loss=keras.losses.SparseCategoricalCrossentropy(\n",
        "                                                              reduction='none'), metrics=[\"accuracy\"]\n",
        "  )\n",
        "  #for i in range(epochs):\n",
        "  hist=model.fit(\n",
        "        [input_tensor, target_tensor],\n",
        "        tf.concat([target_tensor[:,1:],tf.zeros((target_tensor[:,:].shape[0],1))], axis=1),\n",
        "        batch_size=bs,\n",
        "        epochs=epochs,shuffle=True\n",
        "    )\n",
        "    # Save model\n",
        "    # model.save(\"s2s.keras\")\n",
        "    # Run inferencing\n",
        "    # Define sampling models\n",
        "    # Restore the model and construct the encoder and decoder.\n",
        "    #inf = keras.models.load_model(\"/content/s2s.keras\")\n",
        "  encoder_model,decoder_model=build_inference(model,encoder_layers=enc_layer,decoder_layers=dec_layer)\n",
        "    #log train loss to wandb\n",
        "  wandb.log({\"train_loss\": hist.history['loss'][0]})\n",
        "  val_acc=batch_validate(encoder_model,decoder_model,enc_layer,dec_layer)\n",
        "  wandb.log({\"val_acc\":val_acc})"
      ],
      "metadata": {
        "id": "br0HA2vWHe4I"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "if wb:\n",
        "  !wandb login --relogin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv2wpPga3IQd",
        "outputId": "68f3f2b9-79b3-454f-f8b9-d4d586b4ac76"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m API key must be 40 characters long, yours was 94\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    \"name\": \"Bayesian Sweep without attention\",\n",
        "    \"method\":\"bayes\",\n",
        "    \"metric\": {\n",
        "        \"name\": \"val_accuracy\",\n",
        "        \"goal\":\"maximize\"\n",
        "    },\n",
        "    \"parameters\": {\n",
        "        \"cell\": {\"values\": [\"RNN\", \"GRU\", \"LSTM\"]},\n",
        "        \"Embedding\": {\"values\": [32, 15, 10]},\n",
        "        \"Latent\": {\"values\": [512, 256]},\n",
        "        \"Encoder_layer\": {\"values\": [3, 5]},\n",
        "        \"Decoder_layer\": {\"values\": [2, 3, 4]},\n",
        "        \"dropout\": {\"values\": [0, 0.2, 0.3]},\n",
        "        \"epochs\": {\"values\": [10, 15, 20]},\n",
        "        \"Batch_size\": {\"values\": [32, 64, 100]}\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "BvvpeDkB3U9c"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"assignment-4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d46rnXel7x43",
        "outputId": "acd85522-bb69-4358-9851-988133053544"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: gprl2ngx\n",
            "Sweep URL: https://wandb.ai/chandru__n/assignment-4/sweeps/gprl2ngx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(sweep_id, function=train)"
      ],
      "metadata": {
        "id": "Ib-HAUIZ28Pp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c98b615c90164f12a27b6150c7ccbcd6",
            "cdf149a265d44480a28cac7573fbc0d6",
            "ee3b9996386f4be4bc94130557d2786f",
            "e5d91d14374e4fa1b95e81f847d9171b",
            "86e6302e589d41f88510c88bfa378943",
            "1077e278b80b4a14b6fc50fc9a13bbbd",
            "3a68900be4ae4f4587129d4cd98eaf1f",
            "932526099f0e46c3adce6f30b343de7d"
          ]
        },
        "outputId": "c0333596-08ea-4ce3-cf49-3b99aa4bf617"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1ax3rch0 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tBatch_size: 100\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tDecoder_layer: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tEmbedding: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tEncoder_layer: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tLatent: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell: GRU\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220507_152705-1ax3rch0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/chandru__n/assignment-4/runs/1ax3rch0\" target=\"_blank\">comfy-sweep-1</a></strong> to <a href=\"https://wandb.ai/chandru__n/assignment-4\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/chandru__n/assignment-4/sweeps/gprl2ngx\" target=\"_blank\">https://wandb.ai/chandru__n/assignment-4/sweeps/gprl2ngx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "443/443 [==============================] - 57s 88ms/step - loss: 1.1008 - accuracy: 0.7204\n",
            "Epoch 2/20\n",
            "443/443 [==============================] - 39s 87ms/step - loss: 0.8090 - accuracy: 0.7710\n",
            "Epoch 3/20\n",
            "443/443 [==============================] - 39s 87ms/step - loss: 0.6645 - accuracy: 0.8045\n",
            "Epoch 4/20\n",
            "443/443 [==============================] - 38s 87ms/step - loss: 0.5242 - accuracy: 0.8430\n",
            "Epoch 5/20\n",
            "443/443 [==============================] - 39s 87ms/step - loss: 0.3964 - accuracy: 0.8804\n",
            "Epoch 6/20\n",
            "443/443 [==============================] - 39s 87ms/step - loss: 0.2977 - accuracy: 0.9094\n",
            "Epoch 7/20\n",
            "443/443 [==============================] - 39s 87ms/step - loss: 0.2341 - accuracy: 0.9281\n",
            "Epoch 8/20\n",
            "443/443 [==============================] - 39s 87ms/step - loss: 0.1919 - accuracy: 0.9402\n",
            "Epoch 9/20\n",
            "443/443 [==============================] - 39s 87ms/step - loss: 0.1639 - accuracy: 0.9486\n",
            "Epoch 10/20\n",
            "443/443 [==============================] - 39s 87ms/step - loss: 0.1422 - accuracy: 0.9550\n",
            "Epoch 11/20\n",
            "443/443 [==============================] - 39s 87ms/step - loss: 0.1255 - accuracy: 0.9604\n",
            "Epoch 12/20\n",
            "443/443 [==============================] - 39s 87ms/step - loss: 0.1133 - accuracy: 0.9639\n",
            "Epoch 13/20\n",
            "443/443 [==============================] - 39s 87ms/step - loss: 0.1015 - accuracy: 0.9675\n",
            "Epoch 14/20\n",
            "443/443 [==============================] - 38s 87ms/step - loss: 0.0925 - accuracy: 0.9701\n",
            "Epoch 15/20\n",
            "443/443 [==============================] - 39s 87ms/step - loss: 0.0854 - accuracy: 0.9722\n",
            "Epoch 16/20\n",
            "443/443 [==============================] - 39s 87ms/step - loss: 0.0791 - accuracy: 0.9745\n",
            "Epoch 17/20\n",
            "443/443 [==============================] - 39s 87ms/step - loss: 0.0729 - accuracy: 0.9761\n",
            "Epoch 18/20\n",
            "443/443 [==============================] - 39s 87ms/step - loss: 0.0685 - accuracy: 0.9776\n",
            "Epoch 19/20\n",
            "443/443 [==============================] - 39s 88ms/step - loss: 0.0650 - accuracy: 0.9785\n",
            "Epoch 20/20\n",
            "443/443 [==============================] - 39s 87ms/step - loss: 0.0606 - accuracy: 0.9798\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c98b615c90164f12a27b6150c7ccbcd6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>▁</td></tr><tr><td>val_acc</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>1.1008</td></tr><tr><td>val_acc</td><td>0.32974</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">comfy-sweep-1</strong>: <a href=\"https://wandb.ai/chandru__n/assignment-4/runs/1ax3rch0\" target=\"_blank\">https://wandb.ai/chandru__n/assignment-4/runs/1ax3rch0</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220507_152705-1ax3rch0/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rpgiuaix with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tBatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tDecoder_layer: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tEmbedding: 15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tEncoder_layer: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tLatent: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell: RNN\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220507_154127-rpgiuaix</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/chandru__n/assignment-4/runs/rpgiuaix\" target=\"_blank\">deft-sweep-2</a></strong> to <a href=\"https://wandb.ai/chandru__n/assignment-4\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/chandru__n/assignment-4/sweeps/gprl2ngx\" target=\"_blank\">https://wandb.ai/chandru__n/assignment-4/sweeps/gprl2ngx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "118/691 [====>.........................] - ETA: 2:05 - loss: 1.3504 - accuracy: 0.6707"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_type=None\n",
        "embedding_dim=None\n",
        "model= None\n",
        "latent_dim = None\n",
        "enc_layers=None\n",
        "dec_layers=None\n",
        "#this function is needed for training manually\n",
        "def manual_train(config):\n",
        "  global rnn_type\n",
        "  global embedding_dim\n",
        "  global model\n",
        "  global latent_dim\n",
        "  global enc_layer\n",
        "  global dec_layer\n",
        "  rnn_type=config.rnn_type\n",
        "  embedding_dim=config.embedding_dim\n",
        "  latent_dim=config.latent_dim\n",
        "  enc_layer=config.enc_layer\n",
        "  dec_layer=config.dec_layer\n",
        "  dropout=config.dropout\n",
        "  epochs=config.epochs\n",
        "  bs=config.bs\n",
        "  \n",
        "  model=build_model(rnn_type=rnn_type,embedding_dim=embedding_dim,encoder_layers=enc_layer,decoder_layers=dec_layer,dropout=.1)\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=\"adam\", loss=keras.losses.SparseCategoricalCrossentropy(\n",
        "                                                              reduction='none'), metrics=[\"accuracy\"]\n",
        "  )\n",
        "  tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True, show_dtype=True,show_layer_names=True, dpi=96 )\n",
        "  for i in range(epochs):\n",
        "    hist=model.fit(\n",
        "        [input_tensor, target_tensor],\n",
        "        tf.concat([target_tensor[:,1:],tf.zeros((target_tensor[:,:].shape[0],1))], axis=1),\n",
        "        batch_size=bs,\n",
        "        epochs=1,shuffle=True\n",
        "    )\n",
        "\n",
        "    model.save(\"s2s.keras\")\n",
        "\n",
        "    inf = keras.models.load_model(\"/content/s2s.keras\")\n",
        "    encoder_model,decoder_model=build_inference(inf,encoder_layers=enc_layer,decoder_layers=dec_layer)\n",
        "\n",
        "    val_acc=batch_validate(encoder_model,decoder_model,enc_layer,dec_layer)\n",
        "    print(\"Validation Accuracy\",val_acc)\n",
        "  print(\"Test Accuracy\",test_accuracy(encoder_model,decoder_model,enc_layer,dec_layer))"
      ],
      "metadata": {
        "id": "pb4pWLTO2qyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class configuration:\n",
        "  def __init__(self, rnn_type, embedding_dim,latent_dim,enc_layer,dec_layer,dropout,epochs,bs):\n",
        "    self.rnn_type = rnn_type\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.latent_dim = latent_dim\n",
        "    self.enc_layer = enc_layer\n",
        "    self.dec_layer = dec_layer\n",
        "    self.dropout = dropout\n",
        "    self.epochs = epochs\n",
        "    self.bs = bs"
      ],
      "metadata": {
        "id": "lrdTD7Yi2-0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not wb:\n",
        "  config=configuration('LSTM',32,512,3,2,.3,20,64)\n",
        "  manual_train(config)"
      ],
      "metadata": {
        "id": "Nssgxtwd3BOx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}