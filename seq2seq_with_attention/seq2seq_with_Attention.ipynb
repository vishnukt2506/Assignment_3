{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq_with_Attention",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Required libraries importing and installing packages"
      ],
      "metadata": {
        "id": "_8iSk5aSi2Og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install uniseg\n",
        "!pip install wandb --upgrade\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import random\n",
        "import shutil\n",
        "from matplotlib.font_manager import FontProperties\n",
        "from IPython.display import HTML as html_print\n",
        "from IPython.display import display\n",
        "import wandb\n",
        "\n",
        "#!wandb login --relogin"
      ],
      "metadata": {
        "id": "QGiG3lvGnvp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading data set\n"
      ],
      "metadata": {
        "id": "474dgPs2kswL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar --output dakshina.tar\n",
        "!tar -xvf  'dakshina.tar'\n",
        "!wget \"https://github.com/N-Chandru/DeepLearning/raw/main/Nirmala.ttf\""
      ],
      "metadata": {
        "id": "HCyeIyNZpQIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the file paths to train, validation and test dataset\n",
        "train_path = \"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n",
        "vaildation_path = \"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\"\n",
        "test_file_path = \"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\""
      ],
      "metadata": {
        "id": "KWox9k2akl3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataPreprocssing and Tokenising"
      ],
      "metadata": {
        "id": "s2-icELVqTkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataprocessing:\n",
        "  def __init__(self, path) -> None:\n",
        "      self.path = path\n",
        "\n",
        "  def tokeniser(self, language):\n",
        "\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', char_level=True)\n",
        "    tokenizer.fit_on_texts(language)\n",
        "\n",
        "    tensor = tokenizer.texts_to_sequences(language)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "    return tensor, tokenizer\n",
        "  \n",
        "  def dataloading(self):\n",
        "\n",
        "    # creating pairs of target word,input word. E.g. [['\\tअं\\n', '\\tan\\n']\n",
        "    Rows = io.open(self.path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    words = [[ '\\t' + word + '\\n' for word in Row.split('\\t')[:-1]] for Row in Rows[:-1]]\n",
        "    output_lang, input_lang = zip(*words)\n",
        "\n",
        "    input_tensor, input_tokenizer = self.tokeniser(input_lang)\n",
        "    output_tensor, output_tokenizer = self.tokeniser(output_lang)\n",
        "    return input_tensor, input_tokenizer, output_tensor, output_tokenizer\n",
        "\n"
      ],
      "metadata": {
        "id": "s9EhkvxfqwNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading data\n",
        "Data = Dataprocessing(train_path)\n",
        "input_tensor_train, input_lang,  output_tensor_train, output_lang = Data.dataloading()\n",
        "\n",
        "max_length_output, max_length_input = output_tensor_train.shape[1], input_tensor_train.shape[1]\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(output_tensor_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHWXhdTdweuy",
        "outputId": "8f030c43-d7ad-42db-d012-ae9c70bea407"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44203 44203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder class for generating encoder layers"
      ],
      "metadata": {
        "id": "KVNEkKcWUfl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, rnn, vocabulary, embedding, Latent, Batch_size, dropout):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.Batch_size = Batch_size\n",
        "    self.Latent = Latent\n",
        "    self.rnn = rnn\n",
        "    self.Embedding = tf.keras.layers.Embedding(vocabulary, embedding)\n",
        "    self.hidden = tf.zeros((self.Batch_size, self.Latent))\n",
        "    if rnn == 'LSTM':\n",
        "      self.encoder = tf.keras.layers.LSTM(self.Latent, return_sequences=True, \n",
        "                         return_state=True, recurrent_initializer='glorot_uniform',\n",
        "                         dropout = dropout)\n",
        "    elif rnn == 'GRU':\n",
        "      self.encoder = tf.keras.layers.GRU(self.Latent, return_sequences=True,\n",
        "                                   return_state=True, recurrent_initializer='glorot_uniform',\n",
        "                                   dropout = dropout)\n",
        "    else:\n",
        "      self.encoder = tf.keras.layers.SimpleRNN(self.Latent, return_sequences=True, \n",
        "                         return_state=True, recurrent_initializer='glorot_uniform',\n",
        "                         dropout = dropout)\n",
        "      \n",
        "  def call(self, input, hidden, state):\n",
        "    temp = self.Embedding(input)\n",
        "    if self.rnn != 'LSTM':\n",
        "      return self.encoder(temp, initial_state = hidden)\n",
        "    else:\n",
        "      return self.encoder(temp, initial_state = [hidden, state])\n",
        "      "
      ],
      "metadata": {
        "id": "1WTTHqpm3yms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# class for attention layer"
      ],
      "metadata": {
        "id": "xG_seusnUml0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definition of Attention Layer\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, latent):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(latent)\n",
        "    self.W2 = tf.keras.layers.Dense(latent)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, encoder_output):\n",
        "\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    score = self.V(tf.nn.tanh(self.W1(tf.expand_dims(query, 1)) + self.W2(encoder_output)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * encoder_output\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "4L6ZRaM8-JsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# class for decoder layer"
      ],
      "metadata": {
        "id": "InuBuuaYUuSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, rnn, vocabulary, embedding, Latent, Batch_size, dropout):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.Batch_size = Batch_size\n",
        "    self.Latent = Latent\n",
        "    self.rnn = rnn\n",
        "    self.Embedding = tf.keras.layers.Embedding(vocabulary, embedding)\n",
        "    self.hidden = tf.zeros((self.Batch_size, self.Latent))\n",
        "    self.fully_connected = tf.keras.layers.Dense(vocabulary)\n",
        "    self.attention = BahdanauAttention(self.Latent)\n",
        "\n",
        "    if rnn == 'LSTM':\n",
        "      self.decoder = tf.keras.layers.LSTM(self.Latent, return_sequences=True, \n",
        "                         return_state=True, recurrent_initializer='glorot_uniform',\n",
        "                         dropout = dropout)\n",
        "    elif rnn == 'GRU':\n",
        "      self.decoder = tf.keras.layers.GRU(self.Latent, return_sequences=True,\n",
        "                                   return_state=True, recurrent_initializer='glorot_uniform',\n",
        "                                   dropout = dropout)\n",
        "    else:\n",
        "      self.decoder = tf.keras.layers.SimpleRNN(self.Latent, return_sequences=True, \n",
        "                         return_state=True, recurrent_initializer='glorot_uniform',\n",
        "                         dropout = dropout)\n",
        "      \n",
        "  def call(self, input, hidden, encoder_output, state):\n",
        "\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, encoder_output)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    temp = tf.concat([tf.expand_dims(context_vector, 1), self.Embedding(input)], axis=-1)\n",
        "\n",
        "    if self.rnn !='LSTM':\n",
        "      output, state = self.decoder(temp)\n",
        "      temp = self.fully_connected(tf.reshape(output, (-1, output.shape[2])))\n",
        "      return temp, state, attention_weights\n",
        "    \n",
        "    else:\n",
        "      output, last_hidden, state = self.decoder(temp, initial_state=[hidden, state])\n",
        "      temp = self.fully_connected(tf.reshape(output, (-1, output.shape[2])))\n",
        "      return temp, [last_hidden, state], attention_weights\n"
      ],
      "metadata": {
        "id": "7_Xj7eGJB3Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference: https://stackoverflow.com/questions/62916592/loss-function-for-sequences-in-tensorflow-2-0\n",
        "def calculate_loss(real, pred):\n",
        "  position = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss = loss_object(real, pred)\n",
        "\n",
        "  position = tf.cast(position, dtype=loss.dtype)\n",
        "  loss *= position\n",
        "\n",
        "  return tf.reduce_mean(loss)"
      ],
      "metadata": {
        "id": "otqfITgiD1BU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train a model"
      ],
      "metadata": {
        "id": "5NTo8SKDUzUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(use_wandb=True):\n",
        "    \n",
        "    global Batch_size \n",
        "    global latent \n",
        "    global vocab_input_size\n",
        "    global vocab_output_size\n",
        "    global embedding\n",
        "    global encoder\n",
        "    global decoder\n",
        "    global optimizer\n",
        "    global loss_object\n",
        "    global checkpoint_dir\n",
        "    global checkpoint_prefix \n",
        "    global checkpoint\n",
        "    global run_name\n",
        "    global rnn_type\n",
        "\n",
        "    if use_wandb==True:\n",
        "      # initialising the wandb run\n",
        "      run = wandb.init()\n",
        "      # Type of RNN to choose. Acceptable Values are 'RNN'. 'LSTM' and 'GRU'\n",
        "      rnn_type = run.config.cell\n",
        "      # Batch size for training.\n",
        "      Batch_size = run.config.Batch_size\n",
        "      # Dimensions of the abstract representation of the input word and target word.\n",
        "      embedding = run.config.Embedding\n",
        "      # Latent dimensions of the encoder and decoder.\n",
        "      latent = run.config.Latent\n",
        "      # Number of epochs to train for.\n",
        "      epochs = run.config.epochs\n",
        "      #\tFloat between 0 and 1. Denotes the fraction of the units to drop.\n",
        "      dropout = run.config.dropout\n",
        "    else:\n",
        "      rnn_type = 'LSTM'\n",
        "      Batch_size = 64\n",
        "      embedding = 512\n",
        "      latent = 1024\n",
        "      epochs = 20\n",
        "      dropout = 0.2\n",
        "    \n",
        "    BUFFER_SIZE = len(input_tensor_train)\n",
        "    steps_per_epoch = len(input_tensor_train)//Batch_size\n",
        "    vocab_input_size = len(input_lang.word_index)+1\n",
        "    vocab_output_size = len(output_lang.word_index)+1\n",
        "    \n",
        "    run_name = '_epochs_'+str(epochs)+'_rnn_type_'+str(rnn_type)+'_bs_'+str(Batch_size)+'_embed_'+str(embedding)+'_latent_'+str(latent)+'_dropout_'+str(dropout)\n",
        "    if use_wandb==True:\n",
        "      wandb.run.name = run_name\n",
        "\n",
        "    \"\"\" We are using Python iterable object called Dataset.  \n",
        "    The training datapoints are chosen uniformly at random.\"\"\" \n",
        "    dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, output_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "    # We create batches of size BATCH_SIZE and ignore the last batch because the last batch may not be equal to BATCH_SIZE\n",
        "    dataset = dataset.batch(Batch_size, drop_remainder=True)\n",
        "    \n",
        "    \"\"\" Build model\n",
        "    We are explicitly creating a Python iterator using iter and consuming its elements using next. \n",
        "    For Hindi: TensorShape([64, 22]), TensorShape([64, 21]) is the shape of train_input_batch and train_target_batch respectively.\"\"\"\n",
        "    train_input_batch, train_output_batch = next(iter(dataset))\n",
        "    \n",
        "    # encoder and decoder\n",
        "    encoder = Encoder(rnn_type, vocab_input_size, embedding, latent, Batch_size, dropout)\n",
        "    decoder = Decoder(rnn_type, vocab_output_size, embedding, latent, Batch_size, dropout)\n",
        "\n",
        "    if rnn_type != 'LSTM':\n",
        "      output, hidden = encoder(train_input_batch, encoder.hidden, encoder.hidden)\n",
        "      decoder_output, _, _ = decoder(tf.random.uniform((Batch_size, 1)), hidden, output, output)\n",
        "    else:\n",
        "      output, hidden, state = encoder(train_input_batch, encoder.hidden, encoder.hidden)\n",
        "      decoder_output, _, _ = decoder(tf.random.uniform((Batch_size, 1)), hidden, output, state)\n",
        "    \n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "    \n",
        "    train_loss=[0]*epochs\n",
        "    \n",
        "    # Start training\n",
        "    for epoch in range(epochs):\n",
        "      start = time.time()\n",
        "      total_loss = 0\n",
        "      for (batch, (inp, out)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_every_step(inp, out, encoder, decoder, rnn_type)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "      # Storing the average loss per epoch\n",
        "      train_loss[epoch] = total_loss.numpy()/steps_per_epoch\n",
        "      if use_wandb == True:\n",
        "        wandb.log({\"train_loss\": total_loss.numpy()/steps_per_epoch})\n",
        "\n",
        "    val_acc=validate(vaildation_path,rnn_type)\n",
        "    print(\"Train loss: \",train_loss)\n",
        "    print(\"Validation Accuracy: \",val_acc)\n",
        "\n",
        "    if use_wandb ==True:\n",
        "      wandb.log({'val_acc': val_acc})\n",
        "    \n"
      ],
      "metadata": {
        "id": "rxsV1O6wH4tA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_every_step(inp, targ, enocder, decoder,rnn_type):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "        if rnn_type!='LSTM':\n",
        "            enc_output, enc_hidden = encoder(inp, encoder.hidden, encoder.hidden)\n",
        "            dec_hidden = enc_hidden\n",
        "        elif rnn_type=='LSTM':\n",
        "            enc_output, enc_hidden, enc_cell_state = encoder(inp, encoder.hidden, encoder.hidden)\n",
        "            dec_hidden = enc_hidden\n",
        "            dec_cell_state = enc_cell_state\n",
        "        dec_input = tf.expand_dims([output_lang.word_index['\\t']] * Batch_size, 1)\n",
        "        \n",
        "        # Teacher forcing - passing the target as the next input\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            if rnn_type!='LSTM':\n",
        "                # passing enc_output to the decoder\n",
        "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output, enc_output)\n",
        "            elif rnn_type=='LSTM':\n",
        "                if t==1:\n",
        "                  # passing enc_output to the decoder\n",
        "                  predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output,dec_cell_state)\n",
        "                elif t>1:\n",
        "                  predictions, dec_hidden, _ = decoder(dec_input, dec_hidden[0], enc_output,dec_cell_state)\n",
        "            loss += calculate_loss(targ[:, t], predictions)\n",
        "            # using teacher forcing\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "  return batch_loss"
      ],
      "metadata": {
        "id": "v2esE43KPbfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for inference model.\n",
        "def inference_model(input_word,rnn_type):\n",
        "  attention_plot = np.zeros((max_length_output, max_length_input))\n",
        "\n",
        "  input_word = '\\t'+input_word+'\\n'\n",
        "\n",
        "  inputs = [input_lang.word_index[i] for i in input_word]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_input,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  predicted_word = ''\n",
        "  \n",
        "  if rnn_type!='LSTM':\n",
        "    \n",
        "    hidden = [tf.zeros((1, latent))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden, hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "  elif rnn_type=='LSTM':\n",
        "    hidden=tf.zeros((1, latent))\n",
        "    cell_state= tf.zeros((1, latent)) \n",
        "    enc_out, enc_hidden,enc_cell_state = encoder(inputs, hidden, cell_state)\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "  dec_input = tf.expand_dims([output_lang.word_index['\\t']], 0)\n",
        "\n",
        "  att_w=[]\n",
        "\n",
        "  for t in range(max_length_output):\n",
        "    if rnn_type!='LSTM':\n",
        "      predictions, dec_hidden, attention_weights = decoder(dec_input,dec_hidden,enc_out, enc_out)\n",
        "    elif rnn_type=='LSTM':\n",
        "      predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out, enc_cell_state)\n",
        "      dec_hidden=dec_hidden[0]\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "    att_w.append(attention_weights.numpy()[0:len(input_word)])\n",
        "    \n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    predicted_word += output_lang.index_word[predicted_id] \n",
        "\n",
        "    if output_lang.index_word[predicted_id] == '\\n':\n",
        "      return predicted_word, input_word, attention_plot,att_w\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return predicted_word, input_word, attention_plot,att_w"
      ],
      "metadata": {
        "id": "g9kBmFk3R3uN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# does validation"
      ],
      "metadata": {
        "id": "xmAv8DuTi2W7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "def validate(path_to_file,folder_name):\n",
        "  save = False\n",
        "  if path_to_file.find(\"test\")!=-1:\n",
        "    if os.path.exists(os.path.join(os.getcwd(),\"predictions_attention\",str(folder_name))):\n",
        "      shutil.rmtree(os.path.join(os.getcwd(),\"predictions_attention\",str(folder_name)))\n",
        "      \n",
        "    if not os.path.exists(os.path.join(os.getcwd(),\"predictions_attention\")):\n",
        "        os.mkdir(os.path.join(os.getcwd(),\"predictions_attention\"))\n",
        "    os.mkdir(os.path.join(os.getcwd(),\"predictions_attention\",str(folder_name)))\n",
        "    success_file = open(os.path.join(os.getcwd(),\"predictions_attention\",str(folder_name),\"success.txt\"),\"w\",encoding='utf-8', errors='ignore')\n",
        "    failure_file = open(os.path.join(os.getcwd(),\"predictions_attention\",str(folder_name),\"failure.txt\"),\"w\",encoding='utf-8', errors='ignore')\n",
        "    save=True\n",
        "    \n",
        "  success_count=0\n",
        "  # Get the target words and input words for the validation\n",
        "  \n",
        "  Rows = io.open(path_to_file, encoding='UTF-8').read().strip().split('\\n')\n",
        "  words = [[ '\\t' + word + '\\n' for word in Row.split('\\t')[:-1]] for Row in Rows[:-1]]\n",
        "  target_words, input_words = zip(*words)\n",
        "  for i in range(len(input_words)):\n",
        "    predicted_word, input_word, attention_plot,att_w = inference_model(input_words[i],rnn_type)\n",
        "    record= input_word.strip()+' '+target_words[i].strip()+' '+predicted_word[:-1].strip()+\"\\n\"\n",
        "    # The last character of target_words[i] and predicted word is '\\n', first character of target_words[i] is '\\t'\n",
        "    if target_words[i][1:]==predicted_word:\n",
        "      success_count = success_count + 1\n",
        "      if save == True:\n",
        "        success_file.write(record)\n",
        "    elif save==True:\n",
        "      failure_file.write(record)\n",
        "\n",
        "  if save==True:\n",
        "    success_file.close()\n",
        "    failure_file.close()\n",
        "    \n",
        "  return success_count/len(input_words)"
      ],
      "metadata": {
        "id": "8V5rO4GsWlr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# sweep config"
      ],
      "metadata": {
        "id": "0Vu5IAiKjqhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    \"name\": \"Bayesian Sweep with attention2\",\n",
        "    \"method\":\"bayes\",\n",
        "    \"metric\": {\n",
        "        \"name\": \"val_acc\",\n",
        "        \"goal\":\"maximize\"\n",
        "    },\n",
        "    \"parameters\": {\n",
        "        \"cell\": {\"values\": [\"LSTM\", \"GRU\", \"RNN\"]},\n",
        "        \"Embedding\": {\"values\": [512, 256]},\n",
        "        \"Latent\": {\"values\": [1024, 512]},\n",
        "        \"dropout\": {\"values\": [ 0.2, 0, 0.3]},\n",
        "        \"epochs\": {\"values\": [ 20, 25, 30]},\n",
        "        \"Batch_size\": {\"values\": [64, 128]}\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "vToh_8aEjKTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# run below cell for wandb logging"
      ],
      "metadata": {
        "id": "lyTq34OzjgmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"CS6910-Assignment-3\")\n",
        "wandb.agent(sweep_id, train)\n",
        "train(use_wandb=True)"
      ],
      "metadata": {
        "id": "DpBjLhXqjNgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# manuall training"
      ],
      "metadata": {
        "id": "vPOM-ZO7juLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train for manual\n",
        "train(False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yh32lxFX_3p",
        "outputId": "cd6531c3-d027-405e-8967-d969346abceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss:  [0.3808153235394022, 0.13497457089631454, 0.1006564596424932, 0.08152009162349977, 0.06136967202891474, 0.047500317338584126, 0.03963190990945567, 0.03388768071713655, 0.03201230090597401, 0.02814222142316293, 0.026932909868765568, 0.02445453975511634, 0.024141676529594088, 0.02290628267371136, 0.02349087535471156, 0.021879335762797922, 0.022647387739540874, 0.020941703906957654, 0.0196258544921875, 0.019092128587805707]\n",
            "Validation Accuracy:  0.40601331191186596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# plotting function"
      ],
      "metadata": {
        "id": "ko6M9cHLVBBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_attention(attention, input_word, predicted_word, file_name):\n",
        "  hindi_font = FontProperties(fname = os.path.join(os.getcwd(),\"Nirmala.ttf\"))\n",
        "  \n",
        "  fig = plt.figure(figsize=(3, 3))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "  \n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + list(input_word), fontdict=fontdict, rotation=0)\n",
        "  ax.set_yticklabels([''] + list(predicted_word), fontdict=fontdict,fontproperties=hindi_font)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.savefig(file_name)\n",
        "  wandb.init()\n",
        "  wandb.log({\"images\": wandb.Image(fig)})\n",
        "  wandb.finish()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "8Y-Os37qwM9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get html element\n",
        "def cstr(s, color='black'):\n",
        "\tif s == ' ':\n",
        "\t\treturn \"<text style=color:#000;padding-left:10px;background-color:{}> </text>\".format(color, s)\n",
        "\telse:\n",
        "\t\treturn \"<text style=color:#000;background-color:{}>{} </text>\".format(color, s)\n",
        "\t\n",
        "# print html\n",
        "def print_color(t):\n",
        "\tdisplay(html_print(''.join([cstr(ti, color=ci) for ti,ci in t])))\n",
        "\n",
        "# get appropriate color for value\n",
        "# Darker shades of green denotes higher importance.\n",
        "def get_clr(value):\n",
        "\tcolors = ['#85c2e1', '#89c4e2', '#95cae5', '#99cce6', '#a1d0e8',\n",
        "\t\t'#b2d9ec', '#baddee', '#c2e1f0', '#eff7fb', '#f9e8e8',\n",
        "\t\t'#f9e8e8', '#f9d4d4', '#f9bdbd', '#f8a8a8', '#f68f8f',\n",
        "\t\t'#f47676', '#f45f5f', '#f34343', '#f33b3b', '#f42e2e']\n",
        "\tvalue = int((value * 100) / 5)\n",
        "\treturn colors[value]\n",
        "\n",
        "\n",
        "def visualize(input_word, output_word, att_w):\n",
        "  for i in range(len(output_word)):\n",
        "    print(\"\\nOutput character:\", output_word[i], \"\\n\")\n",
        "    text_colours = []\n",
        "    for j in range(len(att_w[i])):\n",
        "      text = (input_word[j], get_clr(att_w[i][j]))\n",
        "      text_colours.append(text)\n",
        "    print_color(text_colours)"
      ],
      "metadata": {
        "id": "HJGADPoFwbVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get appropriate color for value\n",
        "# Darker shades of green denotes higher importance.\n",
        "def get_shade_color(value):\n",
        "\tcolors = ['#00fa00', '#00f500',  '#00eb00', '#00e000',  '#00db00',  \n",
        "           '#00d100',  '#00c700',  '#00c200', '#00b800',  '#00ad00',  \n",
        "           '#00a800',  '#009e00',  '#009400', '#008f00',  '#008500',\n",
        "           '#007500',  '#007000',  '#006600', '#006100',  '#005c00',  \n",
        "           '#005200',  '#004d00',  '#004700', '#003d00',  '#003800',  \n",
        "           '#003300',  '#002900',  '#002400',  '#001f00',  '#001400']\n",
        "\tvalue = int((value * 100) / 5)\n",
        "\treturn colors[value]\n",
        "\n",
        "def create_file(text_colors,input_word,output_word,file_path=os.getcwd()):\n",
        "  text = '''\n",
        "  <!DOCTYPE html>\n",
        "  <html>\n",
        "  <head>\n",
        "    <meta charset=\"UTF-8\"> \n",
        "    <script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js\"></script>\n",
        "    <script>\n",
        "            $(document).ready(function(){\n",
        "            var col =['''\n",
        "  for k in range(3):\n",
        "      for i in range(len(output_word)):\n",
        "              text=text+'''['''\n",
        "              for j in range(len(text_colors[k][i])-1):\n",
        "                text=text+'''\\\"'''+text_colors[k][i][j]+'''\\\"'''+''','''\n",
        "              text=text+'''\\\"'''+text_colors[k][i][len(text_colors[k][i])-1]+'''\\\"'''+'''],'''\n",
        "  text=text[0:-1]\n",
        "  text=text+'''];\\n'''\n",
        "  \n",
        "  for k in range(3):\n",
        "      for i in range(len(output_word[k])):\n",
        "            text=text+'''$(\\\".h'''+str(k)+str(i)+'''\\\").mouseover(function(){\\n'''\n",
        "            for j in range(len(input_word[k])):\n",
        "                       text=text+'''$(\\\".t'''+str(k)+str(j)+'''\\\").css(\\\"background-color\\\", col['''+str(i)+''']'''+'''['''+str(j)+''']);\\n'''\n",
        "            text=text+'''});\\n'''\n",
        "            text=text+'''$(\\\".h'''+str(k)+str(i)+'''\\\").mouseout(function(){\\n'''\n",
        "            for l in range(3):\n",
        "              for j in range(len(input_word[l])):\n",
        "                text=text+'''$(\\\".t'''+str(l)+str(j)+'''\\\").css(\\\"background-color\\\", \\\"#ffff99\\\");\\n'''\n",
        "            text=text+'''});\\n'''\n",
        "  text=text+'''});\\n\n",
        "</script>\n",
        "  </head>\n",
        "      <body>\n",
        "          <h1>Connectivity:</h1>\n",
        "          <p> The connection strength between the target for the selected character and the input characters is highlighted in green (reset). Hover over the text to change the selected character.</p>\n",
        "          <div style=\"background-color:#ffff99;color:black;padding:2%; margin:4%;\">\n",
        "          <p>\n",
        "          <div> Output: </div>\n",
        "          <div style='display:flex; border: 2px solid #d0cccc; padding: 8px; margin: 8px;'>\n",
        "          '''\n",
        "  for k in range(3):\n",
        "      for i in range(len(output_word[k])):\n",
        "            text=text+'''\\n'''+'''\\t'''+'''<div class=\"h'''+str(k)+str(i)+'''\\\">'''+output_word[k][i]+'''</div>'''\n",
        "      text=text+'''</div>'''+'\\n'+'\\t'+'''<div>  </p>'''+'\\n'+'\\t'+'''<p>\n",
        "      <div> Input: </div>\n",
        "      <div style='display:flex; border: 2px solid #d0cccc; padding: 8px; margin: 8px;'>'''    \n",
        "      for j in range(len(input_word[k])):\n",
        "        text=text+'''\\n'''+'''\\t'''+'''<div class=\"t'''+str(k)+str(j)+'''\\\">'''+input_word[k][j]+'''</div>'''\n",
        "      if k<2:\n",
        "          text = text+'''</div></p></div><p></p></div>\n",
        "          <div style=\"background-color:#ffff99;color:black;padding:2%; margin:4%;\">\n",
        "          <div> Output: </div>\n",
        "          <div style='display:flex; border: 2px solid #d0cccc; padding: 8px; margin: 8px;'>'''\n",
        "  text=text+'''\n",
        "        </div>\n",
        "        </p>\n",
        "        </div>\n",
        "        </body>\n",
        "  </html>\n",
        "  '''\n",
        "  fname = os.path.join(file_path,\"connectivity.html\")\n",
        "  file = open(fname,\"w\")\n",
        "  file.write(text)\n",
        "  file.close()\n",
        "\n",
        "def connectivity(input_words,rnn_type,file_path):\n",
        "  color_list=[]\n",
        "  input_word_list=[]\n",
        "  output_word_list=[]\n",
        "  for k in range(3):\n",
        "    output_word, input_word, _ ,att_w = inference_model(input_words[k],rnn_type)\n",
        "    text_colours=[]\n",
        "    for i in range(len(output_word)):\n",
        "      colour=[]\n",
        "      for j in range(len(att_w[i])):\n",
        "        value=get_shade_color(att_w[i][j])\n",
        "        colour.append(value)\n",
        "      text_colours.append(colour)\n",
        "    color_list.append(text_colours)\n",
        "    input_word_list.append(input_word)\n",
        "    output_word_list.append(output_word)\n",
        "  create_file(color_list,input_word_list,output_word_list,file_path)"
      ],
      "metadata": {
        "id": "uXHNenJGweoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transliterate(input_word,rnn_type,file_name=os.path.join(os.getcwd(),\"attention_heatmap.png\"),visual_flag=True):\n",
        "  predicted_word, input_word, attention_plot,att_w = inference_model(input_word,rnn_type)\n",
        "\n",
        "  print(\"\\n\",'Input:', input_word)\n",
        "  print('Predicted transliteration:', predicted_word)\n",
        "\n",
        "  attention_plot = attention_plot[:len(predicted_word),\n",
        "                                  :len(input_word)]\n",
        "  plot_attention(attention_plot, input_word, predicted_word, file_name)\n",
        "\n",
        "  if visual_flag == True:\n",
        "    visualize(input_word, predicted_word, att_w)"
      ],
      "metadata": {
        "id": "SMR0Hz-4wpRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_inputs(rnn_type,n_test_samples=10):\n",
        "  Rows = io.open(test_file_path, encoding='UTF-8').read().strip().split('\\n')\n",
        "  words = [[ '\\t' + word + '\\n' for word in Row.split('\\t')[:-1]] for Row in Rows[:-1]]\n",
        "  target_words, input_words = zip(*words)\n",
        "\n",
        "  for i in range (n_test_samples):\n",
        "    index = random.randint(0,len(input_words))\n",
        "    input_word=input_words[index]\n",
        "    file_name=os.path.join(os.getcwd(),\"predictions_attention\",str(run_name),input_word+\".png\")\n",
        "    \n",
        "    if i == 0:\n",
        "      transliterate(input_word[1:-1],rnn_type, file_name,True)\n",
        "    elif i > 0:\n",
        "      transliterate(input_word[1:-1],rnn_type, file_name,False)"
      ],
      "metadata": {
        "id": "4EpEmUJ_wwtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run for test accuray and plots for test dataa "
      ],
      "metadata": {
        "id": "FS9Z3WJAVF7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login --relogin\n",
        "val_acc=validate(test_file_path,run_name)\n",
        "generate_inputs(rnn_type,10)\n",
        "connectivity(['anjali','underwear','agastya'],rnn_type, os.path.join(os.getcwd(),\"predictions_attention\",str(run_name)))"
      ],
      "metadata": {
        "id": "1obXMYXXx4m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(val_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0FxyrqlyWlg",
        "outputId": "a617f493-8b08-4376-ed1d-105a5a10dccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4056876249722284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# logging visualization"
      ],
      "metadata": {
        "id": "M0Gu5DToVUcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project = 'CS6910-Assignment-3', name = 'html')\n",
        "wandb.log({\"custom_file\": wandb.Html(open('/content/predictions_attention/_epochs_20_rnn_type_LSTM_bs_64_embed_512_latent_1024_dropout_0.2/connectivity.html'))})"
      ],
      "metadata": {
        "id": "Ae489-Zme2E8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}