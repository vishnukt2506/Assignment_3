{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq_without_attention.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# importing required libraries"
      ],
      "metadata": {
        "id": "B0kjQVT9dXP5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6X2Oq_NaZsQK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# downloading dataset"
      ],
      "metadata": {
        "id": "pqN5dOucdd4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!curl https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar --output dakshana.tar\n",
        "!tar -xvf  'dakshana.tar'\n",
        "!pip install wandb\n",
        "import wandb"
      ],
      "metadata": {
        "id": "T3icUU4FZ0--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# setting path variables"
      ],
      "metadata": {
        "id": "lpjp1yWPdyzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path =\"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n",
        "test_path = \"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\" \n",
        "val_path = \"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\""
      ],
      "metadata": {
        "id": "zQtSlE-R7cWM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing of downloaded data and loading"
      ],
      "metadata": {
        "id": "kiAiDy1_d3gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataProcessing():\n",
        "\n",
        "  def __init__(self,train, val, test):\n",
        "    self.train_path = train\n",
        "    self.val_path = val\n",
        "    self.test_path = test\n",
        "    self.tokenizer=None\n",
        "    self.output_tokenizer = None\n",
        "\n",
        "  def load(self, path):\n",
        "    input = []\n",
        "    output = []\n",
        "  \n",
        "    df = pd.read_csv(path,sep=\"\\t\",names=[\"target\", \"input\",\"count\"]).astype(str)\n",
        "    # Add all the  input and target texts with start sequence and end sequence added to target \n",
        "    for index, row in df.iterrows():\n",
        "      inp = row['input']\n",
        "      out = row['target']\n",
        "      out = \"\\t\" + out + \"\\n\"\n",
        "      input.append(inp)\n",
        "      output.append(out)\n",
        "\n",
        "    return input, output\n",
        "  \n",
        "  def process(self):\n",
        "    #only train set will have input_tokenizer as none. Validation and test will will use the same.\n",
        "    train_input, train_output = self.load(self.train_path)\n",
        "    Tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', char_level=True)\n",
        "    Tokenizer.fit_on_texts(train_input)\n",
        "    Target_Tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', char_level=True)\n",
        "    Target_Tokenizer.fit_on_texts(train_output)\n",
        "    self.tokenizer = Tokenizer\n",
        "    self.output_tokenizer= Target_Tokenizer\n",
        "    #pad the text\n",
        "    train_input_tensor = tf.keras.preprocessing.sequence.pad_sequences(Tokenizer.texts_to_sequences(train_input),padding='post')\n",
        "    train_output_tensor = tf.keras.preprocessing.sequence.pad_sequences(Target_Tokenizer.texts_to_sequences(train_output),padding='post')\n",
        "    #for dataset which is not training we pad to make maximum length same as train set.\n",
        "\n",
        "    val_input, val_output = self.load(self.val_path)\n",
        "    val_input_tensor = tf.keras.preprocessing.sequence.pad_sequences(Tokenizer.texts_to_sequences(val_input),padding='post',maxlen=train_input_tensor.shape[1])\n",
        "    val_output_tensor = tf.keras.preprocessing.sequence.pad_sequences(Target_Tokenizer.texts_to_sequences(val_output),padding='post', maxlen=train_output_tensor.shape[1])\n",
        "\n",
        "    test_input, test_output = self.load(self.test_path)\n",
        "    test_input_tensor = tf.keras.preprocessing.sequence.pad_sequences(Tokenizer.texts_to_sequences(test_input),padding='post',maxlen=train_input_tensor.shape[1])\n",
        "    test_output_tensor = tf.keras.preprocessing.sequence.pad_sequences(Target_Tokenizer.texts_to_sequences(test_output),padding='post', maxlen=train_output_tensor.shape[1])\n",
        "    \n",
        "    return train_input_tensor, train_output_tensor, val_input_tensor, val_output_tensor, test_input_tensor, test_output_tensor\n"
      ],
      "metadata": {
        "id": "EDFrDpVyaTzh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# loading data"
      ],
      "metadata": {
        "id": "aC0Vo9oFd_xg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = DataProcessing(train_path, val_path, test_path)\n",
        "train_input, train_output = data.load(train_path)\n",
        "val_input, val_output = data.load(val_path)\n",
        "test_input, test_output = data.load(test_path)\n",
        "train_input_tensor, train_output_tensor, val_input_tensor, val_output_tensor, test_input_tensor, test_output_tensor = data.process()"
      ],
      "metadata": {
        "id": "3ULlasHr4cCP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_encoder_tokens = len(data.tokenizer.word_index)+1\n",
        "num_decoder_tokens = len(data.output_tokenizer.word_index)+1\n",
        "max_encoder_seq_length =  train_input_tensor.shape[1]\n",
        "max_decoder_seq_length = train_output_tensor.shape[1]\n",
        "\n",
        "#convert index to character\n",
        "index_to_char_input = dict((value, key) for key, value in data.tokenizer.word_index.items())\n",
        "index_to_char_target = dict((value, key) for key, value in data.output_tokenizer.word_index.items())"
      ],
      "metadata": {
        "id": "ogGfMnUwDUre"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model class to build a model"
      ],
      "metadata": {
        "id": "ePIrNSrfeJU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model():\n",
        "  def __init__(self, rnn, latent, dropout, embedding):\n",
        "    self.rnn = rnn\n",
        "    self.latent=latent\n",
        "    self.dropout=dropout\n",
        "    self.embedding = embedding\n",
        "\n",
        "  def layer(self):\n",
        "    if self.rnn == \"LSTM\":\n",
        "      return keras.layers.LSTM(self.latent, return_state=True, return_sequences=True,dropout=self.dropout)\n",
        "    elif self.rnn ==\"GRU\":\n",
        "      return keras.layers.GRU(self.latent, return_state=True, return_sequences=True,dropout=self.dropout)\n",
        "    else:\n",
        "      return keras.layers.SimpleRNN(self.latent, return_state=True, return_sequences=True,dropout=self.dropout)\n",
        "\n",
        "  def build(self, encoderlayers, decoderlayers):\n",
        "    #input layer ; takes in tokenize input\n",
        "    encoder_inputs = keras.Input(shape=( max_encoder_seq_length))\n",
        "    #embedding layer\n",
        "    embedded = keras.layers.Embedding(num_encoder_tokens, self.embedding)(encoder_inputs)\n",
        "\n",
        "    if self.rnn == \"LSTM\":\n",
        "      encoder = self.layer()\n",
        "      encoder_outputs, state_h, state_c = encoder(embedded)\n",
        "      for i in range(encoderlayers-1):\n",
        "        encoder = self.layer()\n",
        "        encoder_outputs, state_h, state_c = encoder(encoder_outputs)\n",
        "      encoder_states = [state_h, state_c]\n",
        "\n",
        "    else:\n",
        "      encoder= self.layer()\n",
        "      encoder_outputs, state = encoder(embedded)\n",
        "      for i in range(encoderlayers-1):\n",
        "        encoder = self.layer()\n",
        "        encoder_outputs, state = encoder(encoder_outputs)\n",
        "      encoder_states = [state]\n",
        "\n",
        "    decoder_inputs = keras.Input(shape=( max_decoder_seq_length))\n",
        "    embed = keras.layers.Embedding(num_decoder_tokens, self.embedding)(decoder_inputs)\n",
        "\n",
        "    if self.rnn == \"LSTM\":\n",
        "      decoder = self.layer()\n",
        "      decoder_outputs, state_h, state_c = decoder(embed,initial_state = encoder_states)\n",
        "      for i in range(decoderlayers-1):\n",
        "        decoder = self.layer()\n",
        "        decoder_outputs, state_h, state_c = decoder(decoder_outputs,initial_state = encoder_states)\n",
        "      encoder_states = [state_h, state_c]\n",
        "    else:\n",
        "      decoder= self.layer()\n",
        "      decoder_outputs, state = decoder(embed)\n",
        "      for i in range(decoderlayers-1):\n",
        "        decoder = self.layer()\n",
        "        decoder_outputs, state = decoder(decoder_outputs)\n",
        "      decoder_states = [state]\n",
        "\n",
        "    #Adding dense layer at the end\n",
        "    decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\",name='final')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    #specifying model inputs and outputs.\n",
        "    # encoder_inputs -> Input to encoder\n",
        "    # decoder_inputs -> Input to decoder for teacher forcing\n",
        "    # decoder_outputs -> Output\n",
        "    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "QvKAPZnji3XL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "#building inferencess\n",
        "def build_inference(model,encoder_layers,decoder_layers):\n",
        "    encoder_inputs = model.input[0]  \n",
        "    if isinstance(model.layers[encoder_layers+3], keras.layers.LSTM):\n",
        "      encoder_outputs, state_h, state_c = model.layers[encoder_layers+3].output  \n",
        "      encoder_states = [state_h, state_c]\n",
        "    else:\n",
        "      encoder_outputs, state = model.layers[encoder_layers+3].output  \n",
        "      encoder_states = [state]\n",
        "    encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "    decoder_inputs =  keras.Input(shape=( 1))  \n",
        "    if isinstance(model.layers[encoder_layers+3], keras.layers.LSTM):\n",
        "      decoder_states_inputs=[]\n",
        "      decoder_states=[]\n",
        "      last=None\n",
        "      for i in range(decoder_layers):\n",
        "        #every layer must have an input through which we can supply it's hidden state\n",
        "        decoder_state_input_h = keras.Input(shape=(latent_dim,),name='inp3_'+str(i))\n",
        "        decoder_state_input_c = keras.Input(shape=(latent_dim,),name='inp4_'+str(i))\n",
        "        x = [decoder_state_input_h, decoder_state_input_c]\n",
        "        decoder = model.layers[i+encoder_layers+4]\n",
        "        if i==0:\n",
        "          decoder_outputs, state_h_dec, state_c_dec = decoder(\n",
        "              model.layers[i+encoder_layers+2](decoder_inputs), initial_state=x\n",
        "          )\n",
        "        else:\n",
        "          decoder_outputs, state_h_dec, state_c_dec = decoder(\n",
        "              last, initial_state=x \n",
        "          )\n",
        "        last=decoder_outputs\n",
        "        decoder_states_inputs.append (decoder_state_input_h)\n",
        "        decoder_states_inputs.append (decoder_state_input_c)\n",
        "        decoder_states.append (state_h_dec)\n",
        "        decoder_states.append (state_c_dec)\n",
        "    else:\n",
        "      decoder_states_inputs=[] #Contain all input layers for different hidden state\n",
        "      decoder_states=[] #Contains the hidden states\n",
        "      last=None\n",
        "      for i in range(decoder_layers):\n",
        "        decoder_state_input = keras.Input(shape=(latent_dim,),name='inp3_'+str(i))\n",
        "        x = [decoder_state_input]\n",
        "        decoder = model.layers[i+encoder_layers+4]\n",
        "        if i==0:\n",
        "          decoder_outputs, state = decoder(\n",
        "              model.layers[i+encoder_layers+2](decoder_inputs), initial_state=x\n",
        "          )\n",
        "        else:\n",
        "          decoder_outputs, state = decoder(\n",
        "              last, initial_state=x \n",
        "          )\n",
        "        last=decoder_outputs\n",
        "        decoder_states_inputs.append (decoder_state_input)\n",
        "        decoder_states.append (state)      \n",
        "    decoder_dense = model.get_layer('final')\n",
        "    decoder_outputs = decoder_dense(last)\n",
        "    decoder_model = keras.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        "    )\n",
        "    return encoder_model,decoder_model"
      ],
      "metadata": {
        "id": "qM-cbaDZqwnO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# evaluation class used for validating and accuracy calculation"
      ],
      "metadata": {
        "id": "IOtlMcFkeboZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class evaluation():\n",
        "\n",
        "  def __init__(self, encoder,decoder,encoder_layers,decoder_layers):\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.decoder_layers = decoder_layers\n",
        "    self.encoder_layers= encoder_layers\n",
        "\n",
        "  def decodebatch(self, input, batch_size):\n",
        "      \n",
        "      # Get encoder output\n",
        "      states_value = self.encoder.predict(input)\n",
        "      if rnn_type=='GRU' or 'RNN':\n",
        "        states_value=[states_value]\n",
        "      temp=states_value\n",
        "      for i in range(self.decoder_layers-1):\n",
        "        temp=temp+states_value\n",
        "      states_value=temp\n",
        "      \n",
        "      # This is contain previously predicted character's index for every words in batch.\n",
        "      char_index = np.zeros((batch_size, 1))\n",
        "      # We start with \\t for every word in batch\n",
        "      char_index[:, 0] = data.output_tokenizer.word_index['\\t']\n",
        "      \n",
        "      predicted = [ \"\" for i in range(batch_size)]\n",
        "      done=[False for i in range(batch_size)]\n",
        "      for i in range(max_decoder_seq_length):\n",
        "          out = self.decoder.predict(tuple([char_index] + states_value))\n",
        "          output_prob=out[0]\n",
        "          states_value = out[1:]\n",
        "          for j in range(batch_size):\n",
        "            if done[j]:\n",
        "              continue          \n",
        "            sampled_token_index = np.argmax(output_prob[j, -1, :])\n",
        "            if sampled_token_index == 0:\n",
        "              sampled_char='\\n'\n",
        "            else:\n",
        "              sampled_char = index_to_char_target[sampled_token_index]\n",
        "            if sampled_char == '\\n':\n",
        "              done[j]=True\n",
        "              continue            \n",
        "            predicted[j] += sampled_char\n",
        "            #update the previously predicted characters        \n",
        "            char_index[j,0]=data.output_tokenizer.word_index[sampled_char]\n",
        "      return predicted\n",
        "\n",
        "  def testaccuracy(self):\n",
        "    success=0\n",
        "    #Get all the predicted words\n",
        "    pred=self.decodebatch(test_input_tensor, test_input_tensor.shape[0])\n",
        "    for index in range(test_input_tensor.shape[0]):\n",
        "        predicted = pred[index]\n",
        "        target_word=test_output[index][1:-1]\n",
        "        #test the word one by one and write to files\n",
        "        if target_word == predicted:\n",
        "          success+=1\n",
        "          f = open(\"success.txt\", \"a\")\n",
        "          f.write(test_input[index]+' '+target_word+' '+predicted+'\\n')\n",
        "          f.close()\n",
        "        else:\n",
        "          f = open(\"failure.txt\", \"a\")\n",
        "          f.write(test_input[index]+' '+target_word+' '+predicted+'\\n')\n",
        "          f.close()\n",
        "    return float(success)/float(test_input_tensor.shape[0])\n",
        "\n",
        "  def batchvalidate(self):\n",
        "    success=0\n",
        "    #get all the predicted words\n",
        "    pred = self.decodebatch(val_input_tensor, val_input_tensor.shape[0])\n",
        "    for index in range(val_input_tensor.shape[0]):\n",
        "        predicted = pred[index]\n",
        "        target_word=val_output[index][1:-1]\n",
        "        #test the words one by one\n",
        "        if predicted == target_word:\n",
        "          success+=1\n",
        "    return float(success)/float(val_input_tensor.shape[0])"
      ],
      "metadata": {
        "id": "EaQMiNxjHZe2"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train function"
      ],
      "metadata": {
        "id": "3bKrZup8dUfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_type=None\n",
        "embedding_dim=None\n",
        "model= None\n",
        "latent_dim = None\n",
        "enc_layers=None\n",
        "dec_layers=None\n",
        "def train(wandb=True):\n",
        "  global rnn_type\n",
        "  global embedding_dim\n",
        "  global model\n",
        "  global latent_dim\n",
        "  global enc_layer\n",
        "  global dec_layer\n",
        "\n",
        "  if wandb:\n",
        "    wandb.init()\n",
        "    rnn_type=wandb.config.cell\n",
        "    embedding_dim=wandb.config.Embedding\n",
        "    latent_dim=wandb.config.Latent\n",
        "    enc_layer=wandb.config.Encoder_layer\n",
        "    dec_layer=wandb.config.Decoder_layer\n",
        "    dropout=wandb.config.dropout\n",
        "    epochs=wandb.config.epochs\n",
        "    bs=wandb.config.Batch_size\n",
        "    wandb.run.name = 'epochs_'+str(epochs)+'_bs_'+str(bs)+'_rnn_type_'+str(rnn_type)+'_em_'+str(embedding_dim)+'_latd_'+str(latent_dim)+'_encs_'+str(enc_layer)+'_decs_'+str(dec_layer)+'_dr_'+str(dropout)\n",
        "\n",
        "  else:\n",
        "    rnn_type='LSTM'\n",
        "    embedding_dim=32\n",
        "    latent_dim=256\n",
        "    enc_layer=4\n",
        "    dec_layer=4\n",
        "    dropout=0.3\n",
        "    epochs=20\n",
        "    bs=64\n",
        "\n",
        "  model=Model(rnn_type, latent_dim, dropout, embedding_dim)\n",
        "  model=model.build(enc_layer, dec_layer)\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=\"adam\", loss=keras.losses.SparseCategoricalCrossentropy(\n",
        "                                                              reduction='none'), metrics=[\"accuracy\"]\n",
        "  )\n",
        "  hist=model.fit(\n",
        "        [train_input_tensor, train_output_tensor],\n",
        "        tf.concat([train_output_tensor[:,1:],tf.zeros((train_output_tensor[:,:].shape[0],1))], axis=1),\n",
        "        batch_size=bs,\n",
        "        epochs=epochs,shuffle=True\n",
        "  )\n",
        "\n",
        "  # Run inferencing\n",
        "  encoder_model,decoder_model=build_inference(model,encoder_layers=enc_layer,decoder_layers=dec_layer)\n",
        "  \n",
        "  encoder_model,decoder_model=build_inference(model,encoder_layers=enc_layer,decoder_layers=dec_layer)\n",
        "  eval = evaluation(encoder_model, decoder_model, enc_layer, dec_layer)\n",
        "  val_acc = eval.batchvalidate()\n",
        "  print(\"Validation Accuracy\",val_acc)\n",
        "  if wandb:\n",
        "    #log train loss and val_acc to wandb\n",
        "    wandb.log({\"train_loss\": hist.history['loss'][0]})\n",
        "    wandb.log({\"val_acc\":val_acc})"
      ],
      "metadata": {
        "id": "DYvrdYDctrl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# sweep config"
      ],
      "metadata": {
        "id": "xMYatsAyhjYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    \"name\": \"Bayesian Sweep without attention\",\n",
        "    \"method\":\"bayes\",\n",
        "    \"metric\": {\n",
        "        \"name\": \"val_acc\",\n",
        "        \"goal\":\"maximize\"\n",
        "    },\n",
        "    \"parameters\": {\n",
        "        \"cell\": {\"values\": [\"RNN\", \"GRU\", \"LSTM\"]},\n",
        "        \"Embedding\": {\"values\": [32, 15, 10]},\n",
        "        \"Latent\": {\"values\": [512, 256]},\n",
        "        \"Encoder_layer\": {\"values\": [3, 5]},\n",
        "        \"Decoder_layer\": {\"values\": [2, 3, 4]},\n",
        "        \"dropout\": {\"values\": [0, 0.2, 0.3]},\n",
        "        \"epochs\": {\"values\": [10, 15, 20]},\n",
        "        \"Batch_size\": {\"values\": [32, 64, 100]}\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "z8x2E9CigSCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# to run sweep run this cell"
      ],
      "metadata": {
        "id": "w5OEcqxahmLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"CS6910-Assignment-3\")\n",
        "wandb.agent(sweep_id, function=train)"
      ],
      "metadata": {
        "id": "eGVrjMbNhMQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# manuall train"
      ],
      "metadata": {
        "id": "VyHvlG6WhrQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_type=None\n",
        "embedding_dim=None\n",
        "model= None\n",
        "latent_dim = None\n",
        "enc_layers=None\n",
        "dec_layers=None\n",
        "#this function is needed for training manually\n",
        "def manual_train(config):\n",
        "  global rnn_type\n",
        "  global embedding_dim\n",
        "  global model\n",
        "  global latent_dim\n",
        "  global enc_layer\n",
        "  global dec_layer\n",
        "  rnn_type=config.rnn_type\n",
        "  embedding_dim=config.embedding_dim\n",
        "  latent_dim=config.latent_dim\n",
        "  enc_layer=config.enc_layer\n",
        "  dec_layer=config.dec_layer\n",
        "  dropout=config.dropout\n",
        "  epochs=config.epochs\n",
        "  bs=config.bs\n",
        "  \n",
        "  model=Model(rnn_type, latent_dim, dropout, embedding_dim)\n",
        "  model=model.build(enc_layer, dec_layer)\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=\"adam\", loss=keras.losses.SparseCategoricalCrossentropy(\n",
        "                                                              reduction='none'), metrics=[\"accuracy\"]\n",
        "  )\n",
        "  tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True, show_dtype=True,show_layer_names=True, dpi=96 )\n",
        "  hist=model.fit(\n",
        "        [train_input_tensor, train_output_tensor],\n",
        "        tf.concat([train_output_tensor[:,1:],tf.zeros((train_output_tensor[:,:].shape[0],1))], axis=1),\n",
        "        batch_size=bs,\n",
        "        epochs=epochs,shuffle=True\n",
        "  )\n",
        "  \n",
        "\n",
        "  encoder_model,decoder_model=build_inference(model,encoder_layers=enc_layer,decoder_layers=dec_layer)\n",
        "  eval = evaluation(encoder_model, decoder_model, enc_layer, dec_layer)\n",
        "  val_acc = eval.batchvalidate()\n",
        "  # val_acc=batch_validate(encoder_model,decoder_model,enc_layer,dec_layer)\n",
        "  print(\"Validation Accuracy\",val_acc)\n",
        "  print(\"Test Accuracy\",eval.testaccuracy())"
      ],
      "metadata": {
        "id": "3SbXnGwLLu1A"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class configuration:\n",
        "  def __init__(self, rnn_type, embedding_dim,latent_dim,enc_layer,dec_layer,dropout,epochs,bs):\n",
        "    self.rnn_type = rnn_type\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.latent_dim = latent_dim\n",
        "    self.enc_layer = enc_layer\n",
        "    self.dec_layer = dec_layer\n",
        "    self.dropout = dropout\n",
        "    self.epochs = epochs\n",
        "    self.bs = bs"
      ],
      "metadata": {
        "id": "RqKnDPNOLyUS"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# below cell gives test accuracy for our best model "
      ],
      "metadata": {
        "id": "w767onpmhyw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config=configuration('LSTM',32,256,4,4,.3,20,64)\n",
        "manual_train(config)"
      ],
      "metadata": {
        "id": "Fu1I43dWL0rT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}